{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepQLearning-CartPole-v0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmP6ndbVvzaQr9BZA1ffXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawsifkamal/Deep-Q-Learning/blob/main/DeepQLearning_CartPole_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTBiK1D-6EkA"
      },
      "source": [
        "# Deep Q-Learning with Open AI Gym's Cart Pole Environment\r\n",
        "\r\n",
        "In this notebook, an implementation of the deep-q-learning algorithm will be show step-by-step in order to balance a cartpole given that the AI can move right or left. The agent has to accumilate a reward of 195+ points across 100 episodes in order to win the game. However, because of the restraints of Deep-Q-Learning, the highest reward attained by this model is around 187. Through parameter tuning, more optimal results can be achieved.\r\n",
        "\r\n",
        "To learn more about Deep-Q-Learning, check out my medium article where I provide an intuitive approach to understand this algorithm:\r\n",
        "\r\n",
        " [An Intuitive Approach to Q-learning](https://medium.com/swlh/an-intuitive-approach-to-q-learning-p1-acedb6dff968)\r\n",
        "\r\n",
        "\r\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWRU147J7r4j"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMldCWLEYA8d"
      },
      "source": [
        "%matplotlib inline \r\n",
        "import gym \r\n",
        "import math\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count\r\n",
        "from PIL import Image \r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F \r\n",
        "import torchvision.transforms as T "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GkBjFDKFXxD"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoZCIFUN8OlP"
      },
      "source": [
        "# Installing the Required Dependancies to Render Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJNIn2IjOIRB"
      },
      "source": [
        "%%bash\r\n",
        "\r\n",
        "# install required system dependencies\r\n",
        "apt-get install -y xvfb x11-utils\r\n",
        "\r\n",
        "# install required python dependencies (might need to install additional gym extras depending)\r\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIX563MyOgzE"
      },
      "source": [
        "import pyvirtualdisplay\r\n",
        "\r\n",
        "\r\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\r\n",
        "                                    size=(1400, 900))\r\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ac1EYKLYwCC"
      },
      "source": [
        "is_ipython = 'inline' in matplotlib.get_backend()\r\n",
        "if is_ipython: from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUjTK8Z5Z45D"
      },
      "source": [
        "## Setting up our algorithms -> Classes in function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM1w8cszZ0jl"
      },
      "source": [
        "class DQN(nn.Module): \r\n",
        "  def __init__(self, img_height, img_width):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    # 3 corresponds to 3 colour channels in RGB images that will be received by the NN as inputs \r\n",
        "    self.fc1 = nn.Linear(in_features=img_height * img_width * 3, out_features=24)\r\n",
        "    self.fc2 = nn.Linear(in_features = 24, out_features = 32)\r\n",
        "    self.out = nn.Linear(in_features = 32, out_features = 2)\r\n",
        "\r\n",
        "  def forward(self, t): \r\n",
        "    t = t.flatten(start_dim=1)\r\n",
        "    t = F.relu(self.fc1(t))\r\n",
        "    t = F.relu(self.fc2(t))\r\n",
        "    t = self.out(t)\r\n",
        "    return t \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NlZPU2d3hAY"
      },
      "source": [
        "## Creating the Experience Tuple "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5udfhe-O3skE"
      },
      "source": [
        "Experience = namedtuple(\r\n",
        "    'Experience',\r\n",
        "    ('state', 'action', 'next_state', 'reward')\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_p83aaG4SUh"
      },
      "source": [
        "e = Experience(2, 3, 1, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4_1m4Co4fQz",
        "outputId": "53d79f30-86cb-40bd-c197-052a0bb732db"
      },
      "source": [
        "print(e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experience(state=2, action=3, next_state=1, reward=4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUppNkP84gbE"
      },
      "source": [
        "class ReplayMemory():\r\n",
        "  def __init__(self, capacity):\r\n",
        "    self.capacity  = capacity # defining the capacity for the replay memory lsit \r\n",
        "    self.memory = []\r\n",
        "    self.push_count = 0\r\n",
        "  \r\n",
        "  def push(self, experience):\r\n",
        "    if len(self.memory) < self.capacity: # appending the experience if the lenght of replay memory is less than the capacity that is set \r\n",
        "      self.memory.append(experience)\r\n",
        "    else:\r\n",
        "      #overriding the earliest experience in replay memory with newest experience\r\n",
        "      self.memory[self.push_count % self.capacity] = experience # 0, 1, 2, 3, 4, 5 and then repeat\r\n",
        "    self.push_count += 1 \r\n",
        "\r\n",
        "  def sample(self, batch_size): \r\n",
        "    return random.sample(self.memory, batch_size)\r\n",
        "  \r\n",
        "  def can_provide_sample(self, batch_size):\r\n",
        "    return len(self.memory) >= batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Ct0Z1x3pTq"
      },
      "source": [
        "## Defining the Epsilon Greedy Strategy \r\n",
        "\r\n",
        "This is the strategy is that used for our agent to select an action through the exploration-exploitation trade-off. In this class, the exploration rate for the current time step will be returned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2PD8vo74nqR"
      },
      "source": [
        "class EpsilonGreedyStrategy():\r\n",
        "  def __init__(self, start, end, decay):\r\n",
        "    self.start = start \r\n",
        "    self.end = end \r\n",
        "    self.decay = decay\r\n",
        "\r\n",
        "  def get_exploration_rate(self, current_step):\r\n",
        "    return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT4vDGZs3u33"
      },
      "source": [
        "## Defining the Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhk12tRo7P-m"
      },
      "source": [
        "class Agent():\r\n",
        "  def __init__(self, strategy, num_actions, device):\r\n",
        "    self.current_step = 0\r\n",
        "    self.strategy = strategy\r\n",
        "    self.num_actions = num_actions\r\n",
        "    \r\n",
        "  def select_action(self, state, policy_net):\r\n",
        "    rate = strategy.get_exploration_rate(self.current_step)\r\n",
        "    self.current_step += 1 \r\n",
        "\r\n",
        "    if rate > random.random():\r\n",
        "      action = random.randrange(self.num_actions)\r\n",
        "      return torch.tensor([action]).to(device) # explore \r\n",
        "      \r\n",
        "    else:\r\n",
        "      with torch.no_grad():\r\n",
        "        return policy_net(state).argmax(dim=1).to(device) # exploit # choose the output (qvalues)action with the highest q-value\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjllMPrxeWIp"
      },
      "source": [
        "# Defining the CartPoleEnvManager\r\n",
        "\r\n",
        "This class will contain all of the environment methods and attributes. Including pre-existing gym functionalities, this class also includes image processing for our given states, choosing actions, and extra wrapper functions.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEbtYTjJ8Wtr"
      },
      "source": [
        "class CartPoleEnvManager():\r\n",
        "  def __init__(self, device):\r\n",
        "    self.device = device\r\n",
        "    self.env = gym.make('CartPole-v0').unwrapped \r\n",
        "    self.env.reset()\r\n",
        "    self.current_screen = None \r\n",
        "    self.done = False \r\n",
        "  \r\n",
        "  def reset(self):\r\n",
        "    self.env.reset()\r\n",
        "    self.current_screen = None \r\n",
        "  \r\n",
        "  def close(self):\r\n",
        "    self.env.close()\r\n",
        "  \r\n",
        "  def render(self, mode='human'):\r\n",
        "    return self.env.render(mode)\r\n",
        "  \r\n",
        "  def num_actions_available(self):\r\n",
        "    return self.env.action_space.n \r\n",
        "\r\n",
        "  def take_action(self, action): # action being passed will be a tensor \r\n",
        "    _, reward, self.done, _ = self.env.step(action.item()) # .item() returns the tensor value as a standard python number, which is expected by env.step(x)\r\n",
        "    return torch.tensor([reward], device = self.device) # tensor coming in and tensor coming out; need our reward to be a tensor for later purposes \r\n",
        "  \r\n",
        "  def just_starting(self):\r\n",
        "    return self.current_screen is None # returns true when current screen is one AKA episode is start, or returns false \r\n",
        "  \r\n",
        "  def get_state(self):\r\n",
        "    if self.just_starting() or self.done: # if self.just_starting() == True, then we don't have any previous screen to compare, therefore a starting state == a fully black screen\r\n",
        "      self.current_screen = self.get_processed_screen()\r\n",
        "      black_screen = torch.zeros_like(self.current_screen)\r\n",
        "      return black_screen\r\n",
        "    else:\r\n",
        "      s1 = self.current_screen\r\n",
        "      s2 = self.get_processed_screen()\r\n",
        "      self.current_screen = s2 #s2 becomes current screen and s1 becomes previous screen\r\n",
        "      return s2 - s1\r\n",
        "    \r\n",
        "  def get_screen_height(self):\r\n",
        "    screen = self.get_processed_screen()\r\n",
        "    return screen.shape[2] #2 = height\r\n",
        "    \r\n",
        "  def get_screen_width(self):\r\n",
        "     screen = self.get_processed_screen()\r\n",
        "     return screen.shape[3] # 3 = width\r\n",
        "\r\n",
        "  def get_processed_screen(self):\r\n",
        "     screen = self.render('rgb_array').transpose((2, 0, 1)) # transposes this array into channels by height and by width\r\n",
        "     screen = self.crop_screen(screen)\r\n",
        "     return self.transform_screen_data(screen) # transposed, cropped, and transformed version of the original image is returned by the get_processed_screen method of our EnvManager\r\n",
        "\r\n",
        "  def crop_screen(self, screen):\r\n",
        "     screen_height = screen.shape[1] # getting the height, I thought screen.shape[2] == height????\r\n",
        "     # strip off top and bottom\r\n",
        "     top = int(screen_height * 0.4)\r\n",
        "     bottom = int(screen_height * 0.8)\r\n",
        "     screen = screen[:, top:bottom, :] # stripping off top 40 percent of original image and bottom 20 percent \r\n",
        "     return screen\r\n",
        "\r\n",
        "  def transform_screen_data(self, screen):\r\n",
        "     # Convert to float, rescale, convert to tensor \r\n",
        "     screen = np.ascontiguousarray(screen, dtype=np.float32) /255 # common image preprocessing for neural network input. Contiguosarray == returning all of the stored pixels in the screen as sequential pixels in the contiguous array\r\n",
        "     screen = torch.from_numpy(screen) # convert numpy array to pytorch tensor \r\n",
        "     # use torchvision package to compose image transforms \r\n",
        "     resize = T.Compose([\r\n",
        "                         T.ToPILImage(),\r\n",
        "                         T.Resize((40, 90)), # 40 by 90 image \r\n",
        "                         T.ToTensor() # PIL image is then transformed to a tensor \r\n",
        "     ])\r\n",
        "     return resize(screen).unsqueeze(0).to(self.device) # add a batch dimensions since processed image will be passed to DQN in batches \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEasCder-0wU"
      },
      "source": [
        "# Printing out Unprocessed screen\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "em = CartPoleEnvManager(device)\r\n",
        "em.reset()\r\n",
        "screen = em.render('rgb_array')\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.imshow(screen)\r\n",
        "plt.title('Non-processed screen')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK6RQqSGPGYJ"
      },
      "source": [
        "# Printing out the starting screen\r\n",
        "screen = em.get_state()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0), interpolation='none')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKtSIa-bhB-2"
      },
      "source": [
        "# Printing out the processed screen\r\n",
        "screen = em.get_processed_screen()\r\n",
        "plt.figure()\r\n",
        "plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0), interpolation='none')\r\n",
        "plt.title('Processed screen example')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXr_uTwQQtIr"
      },
      "source": [
        "# Printing out the state after a certain number of episodes have been ran\r\n",
        "em.reset()\r\n",
        "for i in range(6):\r\n",
        "  em.take_action(torch.tensor([1]))\r\n",
        "  screen = em.get_state()\r\n",
        "  \r\n",
        "plt.figure()\r\n",
        "plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0), interpolation='none')\r\n",
        "plt.title('Processed screen example')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpB1o1m4RbSn"
      },
      "source": [
        "# printing out the sceen after the episode has been finished \r\n",
        "em.done = True\r\n",
        "screen = em.get_state()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0), interpolation='none')\r\n",
        "plt.title('Processed screen example')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7xpl5aBtTQt"
      },
      "source": [
        "# Plot the moving average of rewards across 100 episodes \r\n",
        "Moving average period values \r\n",
        "plots the duration of each episode as well as the one hundred episode moving average \r\n",
        "duration for episode is equivalent to the reward of the episode because for each time step the cart and pole env is alive, it will get a reward of one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAI1VXCLSwdf"
      },
      "source": [
        "def plot(values, moving_avg_period):\r\n",
        "  plt.figure(2)\r\n",
        "  plt.clf()\r\n",
        "  plt.title('Training...')\r\n",
        "  plt.xlabel('Episode')\r\n",
        "  plt.ylabel('Duration')\r\n",
        "  plt.plot(values) # values == episode durations\r\n",
        "\r\n",
        "  moving_avg = get_moving_average(moving_avg_period, values)\r\n",
        "  plt.plot(moving_avg)\r\n",
        "  plt.pause(0.001)\r\n",
        "  print('Episode', len(values), '\\n', moving_avg_period, 'episode moving avg:', moving_avg[-1])\r\n",
        "  if is_ipython: display.clear_output(wait=True)\r\n",
        "\r\n",
        "def get_moving_average(period, values): # i.e 100 episode moving average \r\n",
        "  values = torch.tensor(values, dtype=torch.float)\r\n",
        "  if len(values) >= period: \r\n",
        "    moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\r\n",
        "    moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\r\n",
        "    return moving_avg.numpy()\r\n",
        "  \r\n",
        "  else:\r\n",
        "    moving_avg = torch.zeros(len(values))\r\n",
        "    return moving_avg.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psvc7BqY5F9k"
      },
      "source": [
        "## Defining the global Parameters for our Deep-Q-Learning Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JgY1a9JWN99"
      },
      "source": [
        "batch_size = 256\r\n",
        "gamma = 0.999\r\n",
        "eps_start = 1\r\n",
        "eps_end = 0.01\r\n",
        "eps_decay = 0.001\r\n",
        "target_update = 10 # update target networks weights to match policy networks weights every ten episodes\r\n",
        "memory_size = 100000 # capacity of the replay memory \r\n",
        "lr = 0.001 \r\n",
        "num_episodes = 1000\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\r\n",
        "em = CartPoleEnvManager(device)\r\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\r\n",
        "agent = Agent(strategy, em.num_actions_available(), device)\r\n",
        "memory = ReplayMemory(memory_size)\r\n",
        "\r\n",
        "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\r\n",
        "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\r\n",
        "target_net.load_state_dict(policy_net.state_dict()) # setting the wieghts of target net to be equal to the wieghts of the policy net \r\n",
        "target_net.eval()  # not in training mode, only used for inferencing\r\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr = lr) # using the Adam optimizer from the nn.optim class\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THOt2Xn65Rtq"
      },
      "source": [
        "## Extracting each features from the Experience tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URm1Utz2m2T8"
      },
      "source": [
        "# extract tensors function (extract from given experience batch)\r\n",
        "def extract_tensors(experiences): # accepts batch of experience\r\n",
        "  batch = Experience(*zip(*experiences)) # contain numerically readable values from the experience list\r\n",
        "\r\n",
        "  t1 = torch.cat(batch.state)\r\n",
        "  t2 = torch.cat(batch.action)\r\n",
        "  t3 = torch.cat(batch.next_state)\r\n",
        "  t4 = torch.cat(batch.reward)\r\n",
        "\r\n",
        "  return t1, t2, t3, t4 \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtWMfcpr5jNl"
      },
      "source": [
        "## Extracting the Q-values from Policy net and Target net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LD_bkgWzgvJ"
      },
      "source": [
        "class QValues():\r\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "  @staticmethod\r\n",
        "  def get_current(policy_net, states, actions): # state-action pairs sampled from replay memory. so they correspond with each other \r\n",
        "    return policy_net(states).gather(dim = 1, index = actions.unsqueeze(-1)) # predicted q-values for policy net \r\n",
        "\r\n",
        "  @staticmethod \r\n",
        "  def get_next(target_net, next_states):\r\n",
        "    final_state_locations = next_states.flatten(start_dim = 1).max(dim=1)[0].eq(0).type(torch.bool) # final state == True; non-final state == False \r\n",
        "    non_final_state_locations = (final_state_locations == False) # true for non-final state; false for final state\r\n",
        "    non_final_states = next_states[non_final_state_locations]\r\n",
        "    batch_size = next_states.shape[0]\r\n",
        "    values = torch.zeros(batch_size).to(QValues.device)\r\n",
        "    values[non_final_state_locations] = target_net(non_final_states).max(dim = 1)[0].detach() # 0s for the q-values for any final state and contains target_nets maximum predicted q_values across all actions for each_non_final state\r\n",
        "    return values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVUh-Xww5r8P"
      },
      "source": [
        "## Training our agent and printing out rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xB5xCw1lMMr"
      },
      "source": [
        "episode_durations = []\r\n",
        "for episode in range(num_episodes):\r\n",
        "  \r\n",
        "  state = em.get_state() \r\n",
        "\r\n",
        "  for timestep in count():\r\n",
        "    action = agent.select_action(state, policy_net) \r\n",
        "    next_state = em.get_state()\r\n",
        "    memory.push(Experience(state, action, next_state, reward))\r\n",
        "\r\n",
        "\r\n",
        "    if memory.can_provide_sample(batch_size):\r\n",
        "      experiences = memory.sample(batch_size)\r\n",
        "      states, actions, next_states, rewards = extract_tensors(experiences)\r\n",
        "\r\n",
        "      current_q_values = QValues.get_current(policy_net, states, actions) # get current return q-values for any given state action pairs predicted by the policy network\r\n",
        "      next_q_values = QValues.get_next(target_net, next_states) # maximum q-values for the for the next states for the best corresponding actions \r\n",
        "      target_q_values = (next_q_values * gamma) + rewards # bellman equation basically \r\n",
        "\r\n",
        "      loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1)) # current q-values of the current state - the optimal q-values in the next state \r\n",
        "      loss.backward() # calculates gradient of loss function with respect to all weigths and biases of the policy net \r\n",
        "      optimizer.step() # gradient descent \r\n",
        "    \r\n",
        "\r\n",
        "      if em.done:\r\n",
        "        episode_durations.append(timestep)\r\n",
        "        plot(episode_durations, 100)\r\n",
        "        break\r\n",
        "      \r\n",
        "      if episode % target_update == 0:\r\n",
        "        target_net.load_state_dict(policy_net.state_dict())\r\n",
        "\r\n",
        "\r\n",
        "em.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}